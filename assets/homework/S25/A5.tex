\documentclass[12pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.6in} 
\addtolength{\evensidemargin}{-.6in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textwidth}{1.2in}
\addtolength{\textheight}{1.0in}

\renewcommand{\baselinestretch}{1.05}

\usepackage{verbatim,fancyvrb}
\usepackage{soul}
\usepackage{palatino}
\usepackage[dvipsnames]{xcolor}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

\usepackage[final]{graphicx}
\newcommand{\mfigure}[1]{\includegraphics[height=2.5in,
width=3.5in]{#1.eps}}
\newcommand{\regfigure}[2]{\includegraphics[height=#2in,
keepaspectratio=true]{#1.eps}}
\newcommand{\widefigure}[3]{\includegraphics[height=#2in,
width=#3in]{#1.eps}}

\usepackage{amssymb}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=black, citecolor=red, urlcolor=red]{hyperref}

% macros
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1}\quad }

\newcommand{\chapexers}[2]{\prob{Chapter #1, pages #2, Exercises:}}
\newcommand{\exer}[2]{\prob{Exercise #1}}

\newcommand{\pts}[1]{(\emph{#1 pts}) }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)}\quad }
\newcommand{\ppart}[1]{\,\textbf{(#1)}\quad }

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=ellipse,draw,inner sep=2pt] (char) {#1};}}


\begin{document}
\scriptsize \noindent Math 615 NADE (Bueler) \hfill 24 February, 2025
\normalsize

\medskip\bigskip

\Large\centerline{\textbf{Assignment \#5}}
\large
\bigskip

\centerline{\textbf{Due Friday, 7 March 2025, at the start of class}}
\bigskip
\normalsize

\thispagestyle{empty}

\bigskip
Please read textbook\footnote{R.~J.~LeVeque, \emph{Finite Difference Methods for Ordinary and Partial Diff.~Eqns.}, SIAM Press 2007} sections 5.1â€“5.7, plus Appendices C and D.

\bigskip
\noindent \hrulefill

These problems on this Assignment use eigenvalues.  I assume you have already had a course in linear algebra, so this is a review topic.  \textbf{Here is a quick review.}

As usual, $\RR$ denotes the real numbers and $\CC$ the complex numbers.  By definition, a \underline{nonzero} vector $v\in \CC^m$ is an \emph{eigenvector} of a \underline{real} square matrix $A \in \RR^{m\times m}$ if multiplication by $A$ merely lengthens or shortens it:
\begin{equation*}
    A v \stackrel{\ast}{=} \lambda v.
\end{equation*}
The number $\lambda \in \CC$ is called the \emph{eigenvalue} corresponding to $v$.  Note that ``eigen'' means something like ``property of'', so $v$ and $\lambda$ are in some sense owned by $A$.

If $\ast$ holds then the matrix $\lambda I - A$ has a nonzero vector in its null space.  That is, any nonzero multiple of $v$ is sent by $\lambda I - A$ to zero: $(\lambda I - A) v = 0$.  By the fundamental equivalence in linear algebra, $\ast$ holds if and only if $\lambda I - A$ is not invertible.

In particular, $\det(\lambda I - A) = 0$, which is a polynomial equation with real coefficients:
    $$p(\lambda) = \det(\lambda I - A).$$
(Recall we assumed $A$ had real entries.)  Finding all the eigenvalues is equivalent to finding all the roots of the \emph{characteristic polynomial} $p(\lambda)$.  Generally, these roots are complex:
    $$\text{in general, even for real } A \in \RR^{m\times m}, \text{ we have } \lambda\in \CC.$$
However, because the coefficients of the polynomial are real, if $\lambda$ is complex (i.e.~not real) then its conjugate $\bar \lambda$ is also a root of the polynomial and thus an eigenvalue of $A$.  Note that if $A$ is real and $\lambda$ is complex then $v$ must also be complex, since $\ast$ holds.

\medskip
\emph{Fact.}  If $A$ is symmetric $A^\top = A$ then the eigenvalues of $A$ are real.

\medskip
Now suppose $\lambda$ is an eigenvalue of $A$.  Finding a corresponding eigenvector asks to find a vector in the null space of a matrix.  In particular, the row operations of Gauss elimination will convert the equation $(\lambda I - A) v = 0$ into an upper triangular equation $U v = 0$ where $U$ is both upper triangular and has at least one row of zeros.  (\emph{This is because $\lambda I - A$ is not invertible.})  The matrix equation $U v = 0$ can be used to generate every eigenvector corresponding to $\lambda$, the \emph{eigenspace} for $\lambda$.  This eigenspace has dimension at least one.

Appendices C and D cover more advanced eigen-topics.  On this Assignment you will use the basic ideas in section D.3 on matrix exponentials.

\noindent \hrulefill
\clearpage \newpage


\prob{Problem P18.}  \ppart{a}  Compute \emph{by hand} the eigenvalues and eigenvectors of
    $$A = \begin{bmatrix} 0 & -1 & -1 \\
                         -1 & 0  & 1 \\
                         -1 & 1  & 0  \end{bmatrix}.$$
Show all your work.  (\emph{Hint: The characteristic polynomial has integer roots.  You may \emph{check} your work with \Matlab.})

\epart{b}  Continuing with the same matrix $A$, do the following using $\Matlab$ etc., and show the command-line session or code:  Choose a vector $u\in\RR^3$ at random, for instance \texttt{u = randn(3,1)}.  Apply $A$ to it 50 times: $w = A^{50} u$.  Now compute $\|A w\|_2/\|w\|_2$.  You will get the number $2$.  Why?  Explain in several sentences, using equations to make it clear.

\medskip
\noindent \emph{Hint for} \textbf{(b)}.  \emph{The eigenvectors of a symmetric matrix form a basis.  Any vector can be written in this basis.  On the other hand, multiplication by \emph{this} $A$ stretches one basis vector much more than the others.}

\epart{c}  Note that $w = A^{50} u$ from part \textbf{(b)} is very large in norm.  Why?  For a random $u$ and \emph{this} matrix $A$, give an upper bound on the norm of the vector $A^k u$ for large $k$.


\prob{Problem P19.}  \emph{This problem considers the eigenvalues of \emph{non-symmetric} real matrices, which are very frequently seen when solving systems of linear (or linearized) ODEs.  Do not show me the matrices!  Your solution is your code and your percentage estimates.}

\medskip \noindent
Write a short program that computes at least 100 random $3\times 3$ matrices with real entries which are independent, normally-distributed, mean zero, variance one random numbers.  (In \Matlab, \verb| A = randn(3,3) | generates such a matrix.)  For each matrix, compute its eigenvalues numerically---in \Matlab: \verb|eig(A)|---and determine if they are all real or if at least one eigenvalue is complex.  What percentage of such random matrices have exclusively real eigenvalues?  What percentage of such random matrices are symmetric, e.g.~have $\|A-A^\top\|<10^{-10}$?  What percentage of such random matrices are not invertible, e.g.~have $|\det(A)|<10^{-10}$?


\prob{Problem P20.}  \ppart{a} The ODE IVP
    $$v'' = - 9 v, \quad v(0) = v_0, \quad v'(0) = w_0$$
has solution $v(t) = v_0 \cos(3 t) + \frac{w_0}{3} \sin(3t)$.  Verify this.

\epart{b}  Construct the solution a second time by first rewriting the ODE as a first-order system $u' = A u$.  Then compute the solution $u(t) = e^{At} u(0)$ by using equation (D.30) in Appendix D.  Confirm that you get the same result as in \textbf{(a)}.


\prob{Problem P21.}  Check that the solution $u(t)$ given by Duhamel's principle, equation (5.8) in the textbook, satisfies ODE (5.6) and the initial condition $u(t_0)=\eta$.

\medskip
\noindent \emph{Hint.  Look up the Leibniz rule for differentiating an integral.  To understand and explain the simple result of differentiating the matrix exponential, note you can differentiate the absolutely-convergent Taylor series (D.31), in Appendix D, term by term.}


\prob{Problem P22.}  Consider the ODE system
\begin{align*}
u_1' &= 2 u_1, \\
u_2' &= 3 u_1 - 2 u_2
\end{align*}
with initial conditions at $t=0$: $u_1(0) = a, u_2(0) = b$.  Solve this system two ways as follows.

\epart{a} Solve the first equation, using its initial condition.  Insert this into the second equation to get a nonhomogeneous linear ODE for $u_2$.  Solve using Duhamel's principle, equation (5.8) but in the scalar case.

\epart{b} Write the system as $u'=Au$, compute the matrix exponential, and get the solution in the form of equation (D.30) in Appendix D.

\medskip
\noindent \emph{Hints. The diagonalization of $A$ can be done by hand.  Simplify the results of each part sufficiently to see the same solution.}



\prob{Problem P23.}  \emph{This problem is to help you read section 5.2.  You have to deal with notation, and partial derivatives, and matrices, and norms.  The vector $\infty$-norm is defined in equation (A.3).  For an $m\times m$ matrix $A$, the matrix $\infty$-norm is computed by $\|A\|_\infty = \max_{1\le i\le m} \sum_{j=1}^m |a_{ij}|$,
the maximum (absolute) row sum, equation (A.10b).}

\epart{a} For Example 5.1 on pages 113--114, note that the system can be written in first-order system form $u'(t) = f(u(t),t)$ for $u(t) \in \RR^3$.  (The form is given on page 114.)  Compute $f_u(u,t)$, which is the Jacobian matrix. 

\epart{b}  Let $\eta = (0,0,0)^\top$.  Let $\mathcal{D} = \{(u,t)\,:\,\|u - \eta\|_\infty \le 10, 0 \le t \le 1\}$.  Compute the $\infty$-norm Lipschitz constant
	$$L = \max_{(u,t)\in\mathcal{D}} \|f_u(u,t)\|_\infty.$$

\end{document}
